{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ogb\n",
    "# ! python -c \"import ogb; print(ogb.__version__)\"\n",
    "# # Otherwise, please update the version by running\n",
    "\n",
    "# ! pip install torch_geometric\n",
    "# ! pip install --upgrade torch_geometric ogb\n",
    "\n",
    "# ! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/cs276p/lib/python3.12/site-packages/ogb/linkproppred/dataset_pyg.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    }
   ],
   "source": [
    "from ogb.linkproppred import PygLinkPropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Download and process data, stored in './dataset/ogbl_collab/'\n",
    "dataset = PygLinkPropPredDataset(name=\"ogbl-collab\", root='dataset/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/cs276p/lib/python3.12/site-packages/ogb/linkproppred/dataset_pyg.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'train.pt')))\n",
      "/root/miniconda3/envs/cs276p/lib/python3.12/site-packages/ogb/linkproppred/dataset_pyg.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'valid.pt')))\n",
      "/root/miniconda3/envs/cs276p/lib/python3.12/site-packages/ogb/linkproppred/dataset_pyg.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'test.pt')))\n",
      "/root/miniconda3/envs/cs276p/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Get split edges from the dataset\n",
    "split_edge = dataset.get_edge_split()\n",
    "\n",
    "# Extract edge lists for train, valid, and test splits\n",
    "train_edges = split_edge[\"train\"]['edge']  # List of training edges\n",
    "valid_edges = split_edge[\"valid\"]['edge']  # List of validation edges\n",
    "test_edges = split_edge[\"test\"]['edge']    # List of test edges\n",
    "\n",
    "from torch_geometric.loader import Data\n",
    "\n",
    "# Convert to Data objects if necessary\n",
    "train_data = Data(edge_index=train_edges.T)  # Transpose to match PyG format\n",
    "valid_data = Data(edge_index=valid_edges.T)\n",
    "test_data = Data(edge_index=test_edges.T)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader([valid_data], batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6290480494499207\n",
      "Epoch 10, Loss: 0.574454665184021\n",
      "Epoch 20, Loss: 0.5366992950439453\n",
      "Epoch 30, Loss: 0.4889083504676819\n",
      "Epoch 40, Loss: 0.47436967492103577\n",
      "Epoch 50, Loss: 0.4617832899093628\n",
      "Epoch 60, Loss: 0.4526648223400116\n",
      "Epoch 70, Loss: 0.4492711126804352\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a simple GCN-based link prediction model\n",
    "class GCNLinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNLinkPredictor, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x_i, x_j):\n",
    "        # Dot product to predict link probability\n",
    "        return (x_i * x_j).sum(dim=-1)\n",
    "\n",
    "# Load graph data\n",
    "data = dataset[0]  # PyG graph object\n",
    "in_channels = data.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = 32\n",
    "\n",
    "# Instantiate the model and optimizer\n",
    "model = GCNLinkPredictor(in_channels, hidden_channels, out_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    x = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Positive edges\n",
    "    pos_edge_index = train_edges.T\n",
    "    \n",
    "    # Sample negative edges\n",
    "    neg_edge_index = negative_sampling(pos_edge_index, num_nodes=data.num_nodes)\n",
    "    \n",
    "    # Compute link prediction scores for positive and negative edges\n",
    "    pos_pred = model.predict(x[pos_edge_index[0]], x[pos_edge_index[1]])\n",
    "    neg_pred = model.predict(x[neg_edge_index[0]], x[neg_edge_index[1]])\n",
    "    \n",
    "    # Labels for positive and negative edges\n",
    "    pos_label = torch.ones(pos_pred.size(0))\n",
    "    neg_label = torch.zeros(neg_pred.size(0))\n",
    "    \n",
    "    # Concatenate predictions and labels\n",
    "    pred = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "    label = torch.cat([pos_label, neg_label], dim=0)\n",
    "    \n",
    "    # Loss\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(pred, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(100):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Evaluate on validation or test set\n",
    "def evaluate(edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = model(data.x, data.edge_index)\n",
    "        pos_pred = model.predict(x[edge_index[0]], x[edge_index[1]])\n",
    "        return torch.sigmoid(pos_pred)\n",
    "\n",
    "# Predict on validation and test edges\n",
    "valid_scores = evaluate(valid_edges.T)\n",
    "test_scores = evaluate(test_edges.T)\n",
    "\n",
    "print(\"Validation Scores:\", valid_scores)\n",
    "print(\"Test Scores:\", test_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Convert to NetworkX graph for visualization\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Plot original graph with nodes and edges\n",
    "plt.figure(figsize=(10, 10))\n",
    "pos = nx.spring_layout(G, seed=42)  # Use spring layout for visualization\n",
    "\n",
    "# Draw original graph nodes and edges\n",
    "nx.draw_networkx_nodes(G, pos, node_size=50, node_color=\"blue\", alpha=0.7)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "\n",
    "# Visualize the predicted links with high confidence\n",
    "def visualize_predicted_links(edge_index, scores, threshold=0.5):\n",
    "    for i, (u, v) in enumerate(edge_index.T):\n",
    "        # Check if the score exceeds the threshold\n",
    "        if scores[i] > threshold:\n",
    "            # Draw the edge if the score is high\n",
    "            nx.draw_networkx_edges(\n",
    "                G, pos, edgelist=[(u.item(), v.item())], edge_color=\"red\", alpha=0.3, width=2\n",
    "            )\n",
    "\n",
    "# Assume `valid_scores` and `test_scores` contain predicted probabilities for links\n",
    "# Visualize links with high confidence in validation and test sets\n",
    "visualize_predicted_links(valid_edges.T, valid_scores, threshold=0.8)\n",
    "visualize_predicted_links(test_edges.T, test_scores, threshold=0.8)\n",
    "\n",
    "# Show plot\n",
    "plt.title(\"Graph Visualization with Predicted Links\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
