{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 20:10:16.044928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:16.084596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:16.084641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:16.086758: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-17 20:10:16.090615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:16.090664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:16.090679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:17.188014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:17.188081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:17.188086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-17 20:10:17.188112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 20:10:17.188150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5372 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# ! pip install --upgrade torch torch-geometric scikit-learn ogb\n",
    "# ! python -c \"import ogb; print(ogb.__version__)\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from gensim.models import Word2Vec\n",
    "from ogb.linkproppred import PygLinkPropPredDataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "\n",
    "# Download and process data, stored in './dataset/ogbl_collab/'\n",
    "dataset = PygLinkPropPredDataset(name=\"ogbl-collab\", root='dataset/')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Name: ogbl-collab\n",
      "Number of Nodes: 235868\n",
      "Number of Edges: 2358104\n",
      "Number of Node Features: 128\n",
      "Number of Edge Features: None\n"
     ]
    }
   ],
   "source": [
    "# Show dataset information\n",
    "print(\"Dataset Name:\", dataset.name)\n",
    "print(\"Number of Nodes:\", data.num_nodes)\n",
    "print(\"Number of Edges:\", data.edge_index.shape[1])\n",
    "print(\"Number of Node Features:\", data.x.shape[1] if data.x is not None else \"None\")\n",
    "print(\"Number of Edge Features:\", data.edge_attr.shape[1] if data.edge_attr is not None else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split edges into train/validation/test sets\n",
    "split_edge = dataset.get_edge_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> NetworkX graph\n",
    "edge_list = split_edge['train']['edge']\n",
    "graph = nx.Graph()\n",
    "graph.add_edges_from(edge_list.tolist())\n",
    "\n",
    "# NetworkX graph -> StellarGraph\n",
    "stellargraph = StellarGraph.from_networkx(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Node2Vec walks\n",
    "random_walk = BiasedRandomWalk(stellargraph)\n",
    "walks = random_walk.run(\n",
    "    nodes=list(stellargraph.nodes()),  # root nodes\n",
    "    length=10,  # maximum length of a random walk\n",
    "    n=2,  # number of random walks per root node\n",
    "    p=1.0,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "    q=1.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all nodes appear in walks\n",
    "all_nodes = set(str(node) for node in stellargraph.nodes())\n",
    "walked_nodes = set(word for walk in walks for word in walk)\n",
    "missing_nodes = all_nodes - walked_nodes\n",
    "\n",
    "# Add singleton walks for missing nodes\n",
    "walks.extend([[str(node)] for node in missing_nodes])\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(walks, vector_size=128, window=5, min_count=0, sg=1, workers=4, epochs=10)\n",
    "\n",
    "# Extract node embeddings\n",
    "default_embedding = np.zeros(word2vec_model.vector_size)\n",
    "node_embeddings = {\n",
    "    str(node): word2vec_model.wv[str(node)] if str(node) in word2vec_model.wv else default_embedding\n",
    "    for node in stellargraph.nodes()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare edge features\n",
    "def prepare_edge_features_batch(edge_index, embeddings, batch_size=10000):\n",
    "    edge_features = []\n",
    "    num_edges = edge_index.shape[0]\n",
    "    \n",
    "    for start in range(0, num_edges, batch_size):\n",
    "        end = min(start + batch_size, num_edges)\n",
    "        batch = edge_index[start:end]\n",
    "        features = [np.concatenate([embeddings[str(i)], embeddings[str(j)]]) for i, j in batch]\n",
    "        edge_features.extend(features)\n",
    "    \n",
    "    return np.array(edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling\n",
    "def sample_negative_edges(num_nodes, pos_edge_index, num_samples):\n",
    "    neg_edges = []\n",
    "    pos_edge_set = set(map(tuple, pos_edge_index.tolist()))\n",
    "    while len(neg_edges) < num_samples:\n",
    "        i, j = random.randint(0, num_nodes - 1), random.randint(0, num_nodes - 1)\n",
    "        if i != j and (i, j) not in pos_edge_set and (j, i) not in pos_edge_set:\n",
    "            neg_edges.append((i, j))\n",
    "    return np.array(neg_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/validation/test datasets\n",
    "train_pos_edges = split_edge['train']['edge'].numpy()\n",
    "\n",
    "valid_pos_edges = split_edge['valid']['edge'].numpy()\n",
    "valid_neg_edges = split_edge['valid']['edge_neg'].numpy()\n",
    "\n",
    "test_pos_edges = split_edge['test']['edge'].numpy()\n",
    "test_neg_edges = split_edge['test']['edge_neg'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative edges\n",
    "train_neg_edges = sample_negative_edges(data.num_nodes, train_pos_edges, len(train_pos_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate edge features\n",
    "batch_size = 10000  # Adjust this based on available memory\n",
    "\n",
    "train_pos_edges = train_pos_edges[:10000]\n",
    "train_neg_edges = train_neg_edges[:10000]\n",
    "\n",
    "train_pos_features = prepare_edge_features_batch(train_pos_edges, node_embeddings, batch_size)\n",
    "train_neg_features = prepare_edge_features_batch(train_neg_edges, node_embeddings, batch_size)\n",
    "\n",
    "valid_pos_features = prepare_edge_features_batch(valid_pos_edges, node_embeddings, batch_size)\n",
    "valid_neg_features = prepare_edge_features_batch(valid_neg_edges, node_embeddings, batch_size)\n",
    "\n",
    "test_pos_features = prepare_edge_features_batch(test_pos_edges, node_embeddings, batch_size)\n",
    "test_neg_features = prepare_edge_features_batch(test_neg_edges, node_embeddings, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and negative edges\n",
    "train_features = np.vstack([train_pos_features, train_neg_features])\n",
    "train_labels = np.hstack([np.ones(len(train_pos_features)), np.zeros(len(train_neg_features))])\n",
    "\n",
    "valid_features = np.vstack([valid_pos_features, valid_neg_features])\n",
    "valid_labels = np.hstack([np.ones(len(valid_pos_features)), np.zeros(len(valid_neg_features))])\n",
    "\n",
    "test_features = np.vstack([test_pos_features, test_neg_features])\n",
    "test_labels = np.hstack([np.ones(len(test_pos_features)), np.zeros(len(test_neg_features))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Logistic Regression classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.4992\n",
      "Test AUC: 0.4940\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation and test sets\n",
    "valid_preds = classifier.predict_proba(valid_features)[:, 1]\n",
    "valid_auc = roc_auc_score(valid_labels, valid_preds)\n",
    "print(f\"Validation AUC: {valid_auc:.4f}\")\n",
    "\n",
    "test_preds = classifier.predict_proba(test_features)[:, 1]\n",
    "test_auc = roc_auc_score(test_labels, test_preds)\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
